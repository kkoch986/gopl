// Package parser is generated by gogll. Do not edit.
package parser

import (
	"bytes"
	"fmt"
	"sort"
	"strings"

	"github.com/kkoch986/gopl/lexer"
	"github.com/kkoch986/gopl/parser/bsr"
	"github.com/kkoch986/gopl/parser/slot"
	"github.com/kkoch986/gopl/parser/symbols"
	"github.com/kkoch986/gopl/token"
)

type parser struct {
	cI int

	R *descriptors
	U *descriptors

	popped   map[poppedNode]bool
	crf      map[clusterNode][]*crfNode
	crfNodes map[crfNode]*crfNode

	lex         *lexer.Lexer
	parseErrors []*Error

	bsrSet *bsr.Set
}

func newParser(l *lexer.Lexer) *parser {
	return &parser{
		cI:     0,
		lex:    l,
		R:      &descriptors{},
		U:      &descriptors{},
		popped: make(map[poppedNode]bool),
		crf: map[clusterNode][]*crfNode{
			{symbols.NT_StatementList, 0}: {},
		},
		crfNodes:    map[crfNode]*crfNode{},
		bsrSet:      bsr.New(symbols.NT_StatementList, l),
		parseErrors: nil,
	}
}

// Parse returns the BSR set containing the parse forest.
// If the parse was successfull []*Error is nil
func Parse(l *lexer.Lexer) (*bsr.Set, []*Error) {
	return newParser(l).parse()
}

func (p *parser) parse() (*bsr.Set, []*Error) {
	var L slot.Label
	m, cU := len(p.lex.Tokens)-1, 0
	p.ntAdd(symbols.NT_StatementList, 0)
	// p.DumpDescriptors()
	for !p.R.empty() {
		L, cU, p.cI = p.R.remove()

		// fmt.Println()
		// fmt.Printf("L:%s, cI:%d, I[p.cI]:%s, cU:%d\n", L, p.cI, p.lex.Tokens[p.cI], cU)
		// p.DumpDescriptors()

		switch L {
		case slot.Arg0R0: // Arg : ∙string_lit

			p.bsrSet.Add(slot.Arg0R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Arg) {
				p.rtn(symbols.NT_Arg, cU, p.cI)
			} else {
				p.parseError(slot.Arg0R0, p.cI, followSets[symbols.NT_Arg])
			}
		case slot.Arg1R0: // Arg : ∙num_lit

			p.bsrSet.Add(slot.Arg1R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Arg) {
				p.rtn(symbols.NT_Arg, cU, p.cI)
			} else {
				p.parseError(slot.Arg1R0, p.cI, followSets[symbols.NT_Arg])
			}
		case slot.Arg2R0: // Arg : ∙atom

			p.bsrSet.Add(slot.Arg2R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Arg) {
				p.rtn(symbols.NT_Arg, cU, p.cI)
			} else {
				p.parseError(slot.Arg2R0, p.cI, followSets[symbols.NT_Arg])
			}
		case slot.Arg3R0: // Arg : ∙var

			p.bsrSet.Add(slot.Arg3R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Arg) {
				p.rtn(symbols.NT_Arg, cU, p.cI)
			} else {
				p.parseError(slot.Arg3R0, p.cI, followSets[symbols.NT_Arg])
			}
		case slot.Arg4R0: // Arg : ∙Fact

			p.call(slot.Arg4R1, cU, p.cI)
		case slot.Arg4R1: // Arg : Fact ∙

			if p.follow(symbols.NT_Arg) {
				p.rtn(symbols.NT_Arg, cU, p.cI)
			} else {
				p.parseError(slot.Arg4R0, p.cI, followSets[symbols.NT_Arg])
			}
		case slot.Arg5R0: // Arg : ∙List

			p.call(slot.Arg5R1, cU, p.cI)
		case slot.Arg5R1: // Arg : List ∙

			if p.follow(symbols.NT_Arg) {
				p.rtn(symbols.NT_Arg, cU, p.cI)
			} else {
				p.parseError(slot.Arg5R0, p.cI, followSets[symbols.NT_Arg])
			}
		case slot.ArgList0R0: // ArgList : ∙ArgList , Arg

			p.call(slot.ArgList0R1, cU, p.cI)
		case slot.ArgList0R1: // ArgList : ArgList ∙, Arg

			if !p.testSelect(slot.ArgList0R1) {
				p.parseError(slot.ArgList0R1, p.cI, first[slot.ArgList0R1])
				break
			}

			p.bsrSet.Add(slot.ArgList0R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.ArgList0R2) {
				p.parseError(slot.ArgList0R2, p.cI, first[slot.ArgList0R2])
				break
			}

			p.call(slot.ArgList0R3, cU, p.cI)
		case slot.ArgList0R3: // ArgList : ArgList , Arg ∙

			if p.follow(symbols.NT_ArgList) {
				p.rtn(symbols.NT_ArgList, cU, p.cI)
			} else {
				p.parseError(slot.ArgList0R0, p.cI, followSets[symbols.NT_ArgList])
			}
		case slot.ArgList1R0: // ArgList : ∙Arg

			p.call(slot.ArgList1R1, cU, p.cI)
		case slot.ArgList1R1: // ArgList : Arg ∙

			if p.follow(symbols.NT_ArgList) {
				p.rtn(symbols.NT_ArgList, cU, p.cI)
			} else {
				p.parseError(slot.ArgList1R0, p.cI, followSets[symbols.NT_ArgList])
			}
		case slot.Concatenation0R0: // Concatenation : ∙Concatenation , Fact

			p.call(slot.Concatenation0R1, cU, p.cI)
		case slot.Concatenation0R1: // Concatenation : Concatenation ∙, Fact

			if !p.testSelect(slot.Concatenation0R1) {
				p.parseError(slot.Concatenation0R1, p.cI, first[slot.Concatenation0R1])
				break
			}

			p.bsrSet.Add(slot.Concatenation0R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Concatenation0R2) {
				p.parseError(slot.Concatenation0R2, p.cI, first[slot.Concatenation0R2])
				break
			}

			p.call(slot.Concatenation0R3, cU, p.cI)
		case slot.Concatenation0R3: // Concatenation : Concatenation , Fact ∙

			if p.follow(symbols.NT_Concatenation) {
				p.rtn(symbols.NT_Concatenation, cU, p.cI)
			} else {
				p.parseError(slot.Concatenation0R0, p.cI, followSets[symbols.NT_Concatenation])
			}
		case slot.Concatenation1R0: // Concatenation : ∙Fact

			p.call(slot.Concatenation1R1, cU, p.cI)
		case slot.Concatenation1R1: // Concatenation : Fact ∙

			if p.follow(symbols.NT_Concatenation) {
				p.rtn(symbols.NT_Concatenation, cU, p.cI)
			} else {
				p.parseError(slot.Concatenation1R0, p.cI, followSets[symbols.NT_Concatenation])
			}
		case slot.Cons0R0: // Cons : ∙ArgList | ArgList

			p.call(slot.Cons0R1, cU, p.cI)
		case slot.Cons0R1: // Cons : ArgList ∙| ArgList

			if !p.testSelect(slot.Cons0R1) {
				p.parseError(slot.Cons0R1, p.cI, first[slot.Cons0R1])
				break
			}

			p.bsrSet.Add(slot.Cons0R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Cons0R2) {
				p.parseError(slot.Cons0R2, p.cI, first[slot.Cons0R2])
				break
			}

			p.call(slot.Cons0R3, cU, p.cI)
		case slot.Cons0R3: // Cons : ArgList | ArgList ∙

			if p.follow(symbols.NT_Cons) {
				p.rtn(symbols.NT_Cons, cU, p.cI)
			} else {
				p.parseError(slot.Cons0R0, p.cI, followSets[symbols.NT_Cons])
			}
		case slot.Fact0R0: // Fact : ∙atom ()

			p.bsrSet.Add(slot.Fact0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Fact0R1) {
				p.parseError(slot.Fact0R1, p.cI, first[slot.Fact0R1])
				break
			}

			p.bsrSet.Add(slot.Fact0R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Fact) {
				p.rtn(symbols.NT_Fact, cU, p.cI)
			} else {
				p.parseError(slot.Fact0R0, p.cI, followSets[symbols.NT_Fact])
			}
		case slot.Fact1R0: // Fact : ∙string_lit ()

			p.bsrSet.Add(slot.Fact1R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Fact1R1) {
				p.parseError(slot.Fact1R1, p.cI, first[slot.Fact1R1])
				break
			}

			p.bsrSet.Add(slot.Fact1R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Fact) {
				p.rtn(symbols.NT_Fact, cU, p.cI)
			} else {
				p.parseError(slot.Fact1R0, p.cI, followSets[symbols.NT_Fact])
			}
		case slot.Fact2R0: // Fact : ∙atom ( ArgList )

			p.bsrSet.Add(slot.Fact2R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Fact2R1) {
				p.parseError(slot.Fact2R1, p.cI, first[slot.Fact2R1])
				break
			}

			p.bsrSet.Add(slot.Fact2R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Fact2R2) {
				p.parseError(slot.Fact2R2, p.cI, first[slot.Fact2R2])
				break
			}

			p.call(slot.Fact2R3, cU, p.cI)
		case slot.Fact2R3: // Fact : atom ( ArgList ∙)

			if !p.testSelect(slot.Fact2R3) {
				p.parseError(slot.Fact2R3, p.cI, first[slot.Fact2R3])
				break
			}

			p.bsrSet.Add(slot.Fact2R4, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Fact) {
				p.rtn(symbols.NT_Fact, cU, p.cI)
			} else {
				p.parseError(slot.Fact2R0, p.cI, followSets[symbols.NT_Fact])
			}
		case slot.Fact3R0: // Fact : ∙string_lit ( ArgList )

			p.bsrSet.Add(slot.Fact3R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Fact3R1) {
				p.parseError(slot.Fact3R1, p.cI, first[slot.Fact3R1])
				break
			}

			p.bsrSet.Add(slot.Fact3R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Fact3R2) {
				p.parseError(slot.Fact3R2, p.cI, first[slot.Fact3R2])
				break
			}

			p.call(slot.Fact3R3, cU, p.cI)
		case slot.Fact3R3: // Fact : string_lit ( ArgList ∙)

			if !p.testSelect(slot.Fact3R3) {
				p.parseError(slot.Fact3R3, p.cI, first[slot.Fact3R3])
				break
			}

			p.bsrSet.Add(slot.Fact3R4, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Fact) {
				p.rtn(symbols.NT_Fact, cU, p.cI)
			} else {
				p.parseError(slot.Fact3R0, p.cI, followSets[symbols.NT_Fact])
			}
		case slot.FactList0R0: // FactList : ∙FactList , Fact

			p.call(slot.FactList0R1, cU, p.cI)
		case slot.FactList0R1: // FactList : FactList ∙, Fact

			if !p.testSelect(slot.FactList0R1) {
				p.parseError(slot.FactList0R1, p.cI, first[slot.FactList0R1])
				break
			}

			p.bsrSet.Add(slot.FactList0R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.FactList0R2) {
				p.parseError(slot.FactList0R2, p.cI, first[slot.FactList0R2])
				break
			}

			p.call(slot.FactList0R3, cU, p.cI)
		case slot.FactList0R3: // FactList : FactList , Fact ∙

			if p.follow(symbols.NT_FactList) {
				p.rtn(symbols.NT_FactList, cU, p.cI)
			} else {
				p.parseError(slot.FactList0R0, p.cI, followSets[symbols.NT_FactList])
			}
		case slot.FactList1R0: // FactList : ∙Fact

			p.call(slot.FactList1R1, cU, p.cI)
		case slot.FactList1R1: // FactList : Fact ∙

			if p.follow(symbols.NT_FactList) {
				p.rtn(symbols.NT_FactList, cU, p.cI)
			} else {
				p.parseError(slot.FactList1R0, p.cI, followSets[symbols.NT_FactList])
			}
		case slot.List0R0: // List : ∙[]

			p.bsrSet.Add(slot.List0R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_List) {
				p.rtn(symbols.NT_List, cU, p.cI)
			} else {
				p.parseError(slot.List0R0, p.cI, followSets[symbols.NT_List])
			}
		case slot.List1R0: // List : ∙[ Cons ]

			p.bsrSet.Add(slot.List1R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.List1R1) {
				p.parseError(slot.List1R1, p.cI, first[slot.List1R1])
				break
			}

			p.call(slot.List1R2, cU, p.cI)
		case slot.List1R2: // List : [ Cons ∙]

			if !p.testSelect(slot.List1R2) {
				p.parseError(slot.List1R2, p.cI, first[slot.List1R2])
				break
			}

			p.bsrSet.Add(slot.List1R3, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_List) {
				p.rtn(symbols.NT_List, cU, p.cI)
			} else {
				p.parseError(slot.List1R0, p.cI, followSets[symbols.NT_List])
			}
		case slot.List2R0: // List : ∙[ ArgList ]

			p.bsrSet.Add(slot.List2R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.List2R1) {
				p.parseError(slot.List2R1, p.cI, first[slot.List2R1])
				break
			}

			p.call(slot.List2R2, cU, p.cI)
		case slot.List2R2: // List : [ ArgList ∙]

			if !p.testSelect(slot.List2R2) {
				p.parseError(slot.List2R2, p.cI, first[slot.List2R2])
				break
			}

			p.bsrSet.Add(slot.List2R3, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_List) {
				p.rtn(symbols.NT_List, cU, p.cI)
			} else {
				p.parseError(slot.List2R0, p.cI, followSets[symbols.NT_List])
			}
		case slot.Query0R0: // Query : ∙?- Concatenation

			p.bsrSet.Add(slot.Query0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Query0R1) {
				p.parseError(slot.Query0R1, p.cI, first[slot.Query0R1])
				break
			}

			p.call(slot.Query0R2, cU, p.cI)
		case slot.Query0R2: // Query : ?- Concatenation ∙

			if p.follow(symbols.NT_Query) {
				p.rtn(symbols.NT_Query, cU, p.cI)
			} else {
				p.parseError(slot.Query0R0, p.cI, followSets[symbols.NT_Query])
			}
		case slot.Rule0R0: // Rule : ∙Fact :- FactList

			p.call(slot.Rule0R1, cU, p.cI)
		case slot.Rule0R1: // Rule : Fact ∙:- FactList

			if !p.testSelect(slot.Rule0R1) {
				p.parseError(slot.Rule0R1, p.cI, first[slot.Rule0R1])
				break
			}

			p.bsrSet.Add(slot.Rule0R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Rule0R2) {
				p.parseError(slot.Rule0R2, p.cI, first[slot.Rule0R2])
				break
			}

			p.call(slot.Rule0R3, cU, p.cI)
		case slot.Rule0R3: // Rule : Fact :- FactList ∙

			if p.follow(symbols.NT_Rule) {
				p.rtn(symbols.NT_Rule, cU, p.cI)
			} else {
				p.parseError(slot.Rule0R0, p.cI, followSets[symbols.NT_Rule])
			}
		case slot.Statement0R0: // Statement : ∙Query .

			p.call(slot.Statement0R1, cU, p.cI)
		case slot.Statement0R1: // Statement : Query ∙.

			if !p.testSelect(slot.Statement0R1) {
				p.parseError(slot.Statement0R1, p.cI, first[slot.Statement0R1])
				break
			}

			p.bsrSet.Add(slot.Statement0R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Statement) {
				p.rtn(symbols.NT_Statement, cU, p.cI)
			} else {
				p.parseError(slot.Statement0R0, p.cI, followSets[symbols.NT_Statement])
			}
		case slot.Statement1R0: // Statement : ∙Fact .

			p.call(slot.Statement1R1, cU, p.cI)
		case slot.Statement1R1: // Statement : Fact ∙.

			if !p.testSelect(slot.Statement1R1) {
				p.parseError(slot.Statement1R1, p.cI, first[slot.Statement1R1])
				break
			}

			p.bsrSet.Add(slot.Statement1R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Statement) {
				p.rtn(symbols.NT_Statement, cU, p.cI)
			} else {
				p.parseError(slot.Statement1R0, p.cI, followSets[symbols.NT_Statement])
			}
		case slot.Statement2R0: // Statement : ∙Rule .

			p.call(slot.Statement2R1, cU, p.cI)
		case slot.Statement2R1: // Statement : Rule ∙.

			if !p.testSelect(slot.Statement2R1) {
				p.parseError(slot.Statement2R1, p.cI, first[slot.Statement2R1])
				break
			}

			p.bsrSet.Add(slot.Statement2R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Statement) {
				p.rtn(symbols.NT_Statement, cU, p.cI)
			} else {
				p.parseError(slot.Statement2R0, p.cI, followSets[symbols.NT_Statement])
			}
		case slot.StatementList0R0: // StatementList : ∙StatementList Statement

			p.call(slot.StatementList0R1, cU, p.cI)
		case slot.StatementList0R1: // StatementList : StatementList ∙Statement

			if !p.testSelect(slot.StatementList0R1) {
				p.parseError(slot.StatementList0R1, p.cI, first[slot.StatementList0R1])
				break
			}

			p.call(slot.StatementList0R2, cU, p.cI)
		case slot.StatementList0R2: // StatementList : StatementList Statement ∙

			if p.follow(symbols.NT_StatementList) {
				p.rtn(symbols.NT_StatementList, cU, p.cI)
			} else {
				p.parseError(slot.StatementList0R0, p.cI, followSets[symbols.NT_StatementList])
			}
		case slot.StatementList1R0: // StatementList : ∙Statement

			p.call(slot.StatementList1R1, cU, p.cI)
		case slot.StatementList1R1: // StatementList : Statement ∙

			if p.follow(symbols.NT_StatementList) {
				p.rtn(symbols.NT_StatementList, cU, p.cI)
			} else {
				p.parseError(slot.StatementList1R0, p.cI, followSets[symbols.NT_StatementList])
			}

		default:
			panic("This must not happen")
		}
	}
	if !p.bsrSet.Contain(symbols.NT_StatementList, 0, m) {
		p.sortParseErrors()
		return nil, p.parseErrors
	}
	return p.bsrSet, nil
}

func (p *parser) ntAdd(nt symbols.NT, j int) {
	// fmt.Printf("p.ntAdd(%s, %d)\n", nt, j)
	failed := true
	expected := map[token.Type]string{}
	for _, l := range slot.GetAlternates(nt) {
		if p.testSelect(l) {
			p.dscAdd(l, j, j)
			failed = false
		} else {
			for k, v := range first[l] {
				expected[k] = v
			}
		}
	}
	if failed {
		for _, l := range slot.GetAlternates(nt) {
			p.parseError(l, j, expected)
		}
	}
}

/*** Call Return Forest ***/

type poppedNode struct {
	X    symbols.NT
	k, j int
}

type clusterNode struct {
	X symbols.NT
	k int
}

type crfNode struct {
	L slot.Label
	i int
}

/*
suppose that L is Y ::=αX ·β
if there is no CRF node labelled (L,i)
	create one let u be the CRF node labelled (L,i)
if there is no CRF node labelled (X, j) {
	create a CRF node v labelled (X, j)
	create an edge from v to u
	ntAdd(X, j)
} else {
	let v be the CRF node labelled (X, j)
	if there is not an edge from v to u {
		create an edge from v to u
		for all ((X, j,h)∈P) {
			dscAdd(L, i, h);
			bsrAdd(L, i, j, h)
		}
	}
}
*/
func (p *parser) call(L slot.Label, i, j int) {
	// fmt.Printf("p.call(%s,%d,%d)\n", L,i,j)
	u, exist := p.crfNodes[crfNode{L, i}]
	// fmt.Printf("  u exist=%t\n", exist)
	if !exist {
		u = &crfNode{L, i}
		p.crfNodes[*u] = u
	}
	X := L.Symbols()[L.Pos()-1].(symbols.NT)
	ndV := clusterNode{X, j}
	v, exist := p.crf[ndV]
	if !exist {
		// fmt.Println("  v !exist")
		p.crf[ndV] = []*crfNode{u}
		p.ntAdd(X, j)
	} else {
		// fmt.Println("  v exist")
		if !existEdge(v, u) {
			// fmt.Printf("  !existEdge(%v)\n", u)
			p.crf[ndV] = append(v, u)
			// fmt.Printf("|popped|=%d\n", len(popped))
			for pnd := range p.popped {
				if pnd.X == X && pnd.k == j {
					p.dscAdd(L, i, pnd.j)
					p.bsrSet.Add(L, i, j, pnd.j)
				}
			}
		}
	}
}

func existEdge(nds []*crfNode, nd *crfNode) bool {
	for _, nd1 := range nds {
		if nd1 == nd {
			return true
		}
	}
	return false
}

func (p *parser) rtn(X symbols.NT, k, j int) {
	// fmt.Printf("p.rtn(%s,%d,%d)\n", X,k,j)
	pn := poppedNode{X, k, j}
	if _, exist := p.popped[pn]; !exist {
		p.popped[pn] = true
		for _, nd := range p.crf[clusterNode{X, k}] {
			p.dscAdd(nd.L, nd.i, j)
			p.bsrSet.Add(nd.L, nd.i, k, j)
		}
	}
}

// func CRFString() string {
// 	buf := new(bytes.Buffer)
// 	buf.WriteString("CRF: {")
// 	for cn, nds := range crf{
// 		for _, nd := range nds {
// 			fmt.Fprintf(buf, "%s->%s, ", cn, nd)
// 		}
// 	}
// 	buf.WriteString("}")
// 	return buf.String()
// }

func (cn clusterNode) String() string {
	return fmt.Sprintf("(%s,%d)", cn.X, cn.k)
}

func (n crfNode) String() string {
	return fmt.Sprintf("(%s,%d)", n.L.String(), n.i)
}

// func PoppedString() string {
// 	buf := new(bytes.Buffer)
// 	buf.WriteString("Popped: {")
// 	for p, _ := range popped {
// 		fmt.Fprintf(buf, "(%s,%d,%d) ", p.X, p.k, p.j)
// 	}
// 	buf.WriteString("}")
// 	return buf.String()
// }

/*** descriptors ***/

type descriptors struct {
	set []*descriptor
}

func (ds *descriptors) contain(d *descriptor) bool {
	for _, d1 := range ds.set {
		if d1 == d {
			return true
		}
	}
	return false
}

func (ds *descriptors) empty() bool {
	return len(ds.set) == 0
}

func (ds *descriptors) String() string {
	buf := new(bytes.Buffer)
	buf.WriteString("{")
	for i, d := range ds.set {
		if i > 0 {
			buf.WriteString("; ")
		}
		fmt.Fprintf(buf, "%s", d)
	}
	buf.WriteString("}")
	return buf.String()
}

type descriptor struct {
	L slot.Label
	k int
	i int
}

func (d *descriptor) String() string {
	return fmt.Sprintf("%s,%d,%d", d.L, d.k, d.i)
}

func (p *parser) dscAdd(L slot.Label, k, i int) {
	// fmt.Printf("p.dscAdd(%s,%d,%d)\n", L, k, i)
	d := &descriptor{L, k, i}
	if !p.U.contain(d) {
		p.R.set = append(p.R.set, d)
		p.U.set = append(p.U.set, d)
	}
}

func (ds *descriptors) remove() (L slot.Label, k, i int) {
	d := ds.set[len(ds.set)-1]
	ds.set = ds.set[:len(ds.set)-1]
	// fmt.Printf("remove: %s,%d,%d\n", d.L, d.k, d.i)
	return d.L, d.k, d.i
}

func (p *parser) DumpDescriptors() {
	p.DumpR()
	p.DumpU()
}

func (p *parser) DumpR() {
	fmt.Println("R:")
	for _, d := range p.R.set {
		fmt.Printf(" %s\n", d)
	}
}

func (p *parser) DumpU() {
	fmt.Println("U:")
	for _, d := range p.U.set {
		fmt.Printf(" %s\n", d)
	}
}

/*** TestSelect ***/

func (p *parser) follow(nt symbols.NT) bool {
	_, exist := followSets[nt][p.lex.Tokens[p.cI].Type()]
	return exist
}

func (p *parser) testSelect(l slot.Label) bool {
	_, exist := first[l][p.lex.Tokens[p.cI].Type()]
	// fmt.Printf("testSelect(%s) = %t\n", l, exist)
	return exist
}

var first = []map[token.Type]string{
	// Arg : ∙string_lit
	{
		token.T_12: "string_lit",
	},
	// Arg : string_lit ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Arg : ∙num_lit
	{
		token.T_11: "num_lit",
	},
	// Arg : num_lit ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Arg : ∙atom
	{
		token.T_10: "atom",
	},
	// Arg : atom ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Arg : ∙var
	{
		token.T_13: "var",
	},
	// Arg : var ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Arg : ∙Fact
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Arg : Fact ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Arg : ∙List
	{
		token.T_7: "[",
		token.T_8: "[]",
	},
	// Arg : List ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// ArgList : ∙ArgList , Arg
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// ArgList : ArgList ∙, Arg
	{
		token.T_3: ",",
	},
	// ArgList : ArgList , ∙Arg
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// ArgList : ArgList , Arg ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// ArgList : ∙Arg
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// ArgList : Arg ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Concatenation : ∙Concatenation , Fact
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Concatenation : Concatenation ∙, Fact
	{
		token.T_3: ",",
	},
	// Concatenation : Concatenation , ∙Fact
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Concatenation : Concatenation , Fact ∙
	{
		token.T_3: ",",
		token.T_4: ".",
	},
	// Concatenation : ∙Fact
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Concatenation : Fact ∙
	{
		token.T_3: ",",
		token.T_4: ".",
	},
	// Cons : ∙ArgList | ArgList
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// Cons : ArgList ∙| ArgList
	{
		token.T_14: "|",
	},
	// Cons : ArgList | ∙ArgList
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// Cons : ArgList | ArgList ∙
	{
		token.T_9: "]",
	},
	// Fact : ∙atom ()
	{
		token.T_10: "atom",
	},
	// Fact : atom ∙()
	{
		token.T_1: "()",
	},
	// Fact : atom () ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_4:  ".",
		token.T_5:  ":-",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Fact : ∙string_lit ()
	{
		token.T_12: "string_lit",
	},
	// Fact : string_lit ∙()
	{
		token.T_1: "()",
	},
	// Fact : string_lit () ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_4:  ".",
		token.T_5:  ":-",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Fact : ∙atom ( ArgList )
	{
		token.T_10: "atom",
	},
	// Fact : atom ∙( ArgList )
	{
		token.T_0: "(",
	},
	// Fact : atom ( ∙ArgList )
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// Fact : atom ( ArgList ∙)
	{
		token.T_2: ")",
	},
	// Fact : atom ( ArgList ) ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_4:  ".",
		token.T_5:  ":-",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Fact : ∙string_lit ( ArgList )
	{
		token.T_12: "string_lit",
	},
	// Fact : string_lit ∙( ArgList )
	{
		token.T_0: "(",
	},
	// Fact : string_lit ( ∙ArgList )
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// Fact : string_lit ( ArgList ∙)
	{
		token.T_2: ")",
	},
	// Fact : string_lit ( ArgList ) ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_4:  ".",
		token.T_5:  ":-",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// FactList : ∙FactList , Fact
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// FactList : FactList ∙, Fact
	{
		token.T_3: ",",
	},
	// FactList : FactList , ∙Fact
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// FactList : FactList , Fact ∙
	{
		token.T_3: ",",
		token.T_4: ".",
	},
	// FactList : ∙Fact
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// FactList : Fact ∙
	{
		token.T_3: ",",
		token.T_4: ".",
	},
	// List : ∙[]
	{
		token.T_8: "[]",
	},
	// List : [] ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// List : ∙[ Cons ]
	{
		token.T_7: "[",
	},
	// List : [ ∙Cons ]
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// List : [ Cons ∙]
	{
		token.T_9: "]",
	},
	// List : [ Cons ] ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// List : ∙[ ArgList ]
	{
		token.T_7: "[",
	},
	// List : [ ∙ArgList ]
	{
		token.T_7:  "[",
		token.T_8:  "[]",
		token.T_10: "atom",
		token.T_11: "num_lit",
		token.T_12: "string_lit",
		token.T_13: "var",
	},
	// List : [ ArgList ∙]
	{
		token.T_9: "]",
	},
	// List : [ ArgList ] ∙
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Query : ∙?- Concatenation
	{
		token.T_6: "?-",
	},
	// Query : ?- ∙Concatenation
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Query : ?- Concatenation ∙
	{
		token.T_4: ".",
	},
	// Rule : ∙Fact :- FactList
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Rule : Fact ∙:- FactList
	{
		token.T_5: ":-",
	},
	// Rule : Fact :- ∙FactList
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Rule : Fact :- FactList ∙
	{
		token.T_4: ".",
	},
	// Statement : ∙Query .
	{
		token.T_6: "?-",
	},
	// Statement : Query ∙.
	{
		token.T_4: ".",
	},
	// Statement : Query . ∙
	{
		token.EOF:  "$",
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Statement : ∙Fact .
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Statement : Fact ∙.
	{
		token.T_4: ".",
	},
	// Statement : Fact . ∙
	{
		token.EOF:  "$",
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Statement : ∙Rule .
	{
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// Statement : Rule ∙.
	{
		token.T_4: ".",
	},
	// Statement : Rule . ∙
	{
		token.EOF:  "$",
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// StatementList : ∙StatementList Statement
	{
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// StatementList : StatementList ∙Statement
	{
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// StatementList : StatementList Statement ∙
	{
		token.EOF:  "$",
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// StatementList : ∙Statement
	{
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// StatementList : Statement ∙
	{
		token.EOF:  "$",
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
}

var followSets = []map[token.Type]string{
	// Arg
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// ArgList
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Concatenation
	{
		token.T_3: ",",
		token.T_4: ".",
	},
	// Cons
	{
		token.T_9: "]",
	},
	// Fact
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_4:  ".",
		token.T_5:  ":-",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// FactList
	{
		token.T_3: ",",
		token.T_4: ".",
	},
	// List
	{
		token.T_2:  ")",
		token.T_3:  ",",
		token.T_9:  "]",
		token.T_14: "|",
	},
	// Query
	{
		token.T_4: ".",
	},
	// Rule
	{
		token.T_4: ".",
	},
	// Statement
	{
		token.EOF:  "$",
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
	// StatementList
	{
		token.EOF:  "$",
		token.T_6:  "?-",
		token.T_10: "atom",
		token.T_12: "string_lit",
	},
}

/*** Errors ***/

/*
Error is returned by Parse at every point at which the parser fails to parse
a grammar production. For non-LL-1 grammars there will be an error for each
alternate attempted by the parser.

The errors are sorted in descending order of input position (index of token in
the stream of tokens).

Normally the error of interest is the one that has parsed the largest number of
tokens.
*/
type Error struct {
	// Index of token that caused the error.
	cI int

	// Grammar slot at which the error occured.
	Slot slot.Label

	// The token at which the error occurred.
	Token *token.Token

	// The line and column in the input text at which the error occurred
	Line, Column int

	// The tokens expected at the point where the error occurred
	Expected map[token.Type]string
}

func (pe *Error) String() string {
	w := new(bytes.Buffer)
	fmt.Fprintf(w, "Parse Error: %s I[%d]=%s at line %d col %d\n",
		pe.Slot, pe.cI, pe.Token, pe.Line, pe.Column)
	exp := []string{}
	for _, e := range pe.Expected {
		exp = append(exp, e)
	}
	fmt.Fprintf(w, "Expected one of: [%s]", strings.Join(exp, ","))
	return w.String()
}

func (p *parser) parseError(slot slot.Label, i int, expected map[token.Type]string) {
	pe := &Error{cI: i, Slot: slot, Token: p.lex.Tokens[i], Expected: expected}
	p.parseErrors = append(p.parseErrors, pe)
}

func (p *parser) sortParseErrors() {
	sort.Slice(p.parseErrors,
		func(i, j int) bool {
			return p.parseErrors[j].Token.Lext() < p.parseErrors[i].Token.Lext()
		})
	for _, pe := range p.parseErrors {
		pe.Line, pe.Column = p.lex.GetLineColumn(pe.Token.Lext())
	}
}
